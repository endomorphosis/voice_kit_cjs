{"version":3,"file":"background.js","mappings":"UAAIA,ECCAC,EADAC,E,2CCGJ,MAAMC,EAAS,CACXC,KAAM,oDACNC,IAAK,kCAGFC,eAAeC,EAAkBC,SAChBC,OAAOC,KAAK,kBAEhC,IAAK,MAAOC,EAAMC,KAAYC,OAAOC,QAAQX,GACzC,IACIY,QAAQC,IAAI,mBAAmBL,YAAeC,WAGrBK,EAAAA,EAAAA,KACZ,QAATN,EAAiB,+BAAiC,kBAClDC,EACA,CACIM,UAAW,SACXC,kBAAkB,EAClBC,kBAAoBC,IACZb,GACAA,EAAiB,CACbc,MAAOX,KACJU,GAEX,IAKZN,QAAQC,IAAI,uBAAuBL,UAEvC,CAAE,MAAOY,GACLR,QAAQQ,MAAM,yBAAyBZ,WAAeY,EAC1D,CAER,CAGOjB,eAAekB,IAClB,IACI,MAAMC,QAAchB,OAAOC,KAAK,kBAOhC,aANqBgB,QAAQC,IACzBd,OAAOe,OAAOzB,GAAQ0B,KAAIvB,gBACFmB,EAAMK,KAAK,UAAUlB,QAC5BmB,OAAS,MAGhBC,MAAMC,QACxB,CAAE,MAAOV,GAEL,OADAR,QAAQQ,MAAM,8BAA+BA,IACtC,CACX,CACJ,CCxDO,MAAMW,EAAuB,0BCa9BC,EAAoB,IAAIC,EAAAA,IAE9B,IAAIC,EAAc,gBACdC,EAAkB,EAQtB,SAASC,EAAgBC,EAAQnB,EAAO,MAErB,YAAXmB,GAAwBnB,GAAMmB,QAC9BzB,QAAQC,IAAI,KAAI,IAAIyB,MAAOC,mCAAoCrB,GAEnEsB,OAAOC,QAAQC,YAAY,CACvBL,SACAnB,OACAyB,SAAUR,IACXS,OAAM,QAGb,CAKA,MAAMC,EACJC,gBAAkB,oDAClBA,gBAAkB,KAClBA,gBAAkBR,KAAKS,MACvBD,6BAA+B,IAE/B,wBAAaE,CAAY/B,EAAoB,MAC3C,MAAM8B,EAAMT,KAAKS,MAGjB,GAAIE,KAAKC,UAAaH,EAAME,KAAKE,SAAWF,KAAKG,sBAAwB,CACvExC,QAAQC,IAAI,oDACZ,IAEMoC,KAAKC,SAAS/B,OAAOkC,eACjBJ,KAAKC,SAAS/B,MAAMkC,UAExBJ,KAAKC,SAASI,WAAWD,eACrBJ,KAAKC,SAASI,UAAUD,UAEhCJ,KAAKC,SAAW,KAEZK,EAAAA,EAAOC,IACTD,EAAAA,EAAOC,IAEX,CAAE,MAAOpC,GACPR,QAAQ6C,KAAK,2BAA4BrC,EAC3C,CACF,CAEA,GAAI6B,KAAKC,SAGP,OAFAD,KAAKE,SAAWJ,EAChBX,EAAgB,SACTa,KAAKC,SAGd,IACEtC,QAAQ8C,KAAK,qBACbxB,EAAc,UACdE,EAAgB,WAEhB,MAAMuB,EAAkBzC,SACE0C,IAAlB1C,EAAKyB,WACLR,EAAkBjB,EAAKyB,SACvB/B,QAAQC,IAAI,KAAI,IAAIyB,MAAOC,+BAAgC,CACvDsB,KAAM3C,EAAK2C,KACXxB,OAAQnB,EAAKmB,OACbM,SAAUzB,EAAKyB,WAEnBP,EAAgB,UAAWlB,IAE3BD,GAAmBA,EAAkBC,EAAI,EAIjD,IAEI,GADAN,QAAQ8C,KAAK,eACRI,UAAUC,IACX,MAAM,IAAIC,MAAM,2CAGpB,MAAMC,QAAgBH,UAAUC,IAAIG,iBACpC,IAAKD,EACD,MAAM,IAAID,MAAM,gCAIpB,IAAIG,EACAC,EAAW,GACf,IACED,QAAeF,EAAQI,cAAc,CACnCC,iBAAkB,CAAC,gBAErBF,EAASG,KAAK,aAChB,CAAE,MAAOC,GACP5D,QAAQC,IAAI,oDACZsD,QAAeF,EAAQI,eACzB,CAGA,MAAMI,EAAY,CAChBC,4BAA6BC,KAAKC,IAChCX,EAAQY,OAAOH,4BACf,YAEFI,cAAeH,KAAKC,IAClBX,EAAQY,OAAOC,cACf,aAIJlE,QAAQC,IAAI,wBAAyB,CACnCkE,KAAMd,EAAQc,KACdX,SAAUY,MAAMC,KAAKhB,EAAQG,UAC7BS,OAAQJ,IAIV,MAAMS,EAAa,CACjBC,mBAAoB,CAAC,SAAU,OAC/BC,uBAAwB,GACxBC,qBAAqB,EACrBC,cAAe,aACfC,mBAAmB,EACnBC,OAAQ,CACNC,UAAWtB,EACXuB,gBAAiB,OACjBC,WAAY,WACZC,kBAAkB,EAClBC,eAAe,IAKfzB,EAAS0B,SAAS,gBACpBZ,EAAWM,OAAOO,gBAAkB,CAAC,QAGvCnF,QAAQoF,QAAQ,cAEhBpF,QAAQ8C,KAAK,oBACbT,KAAKK,gBAAkB2C,EAAAA,IAAcC,gBAAgBjD,KAAKkD,SAAU,CAChElF,kBAAmB0C,EACnB5C,UAAW,SACXC,kBAAkB,EAClBoF,YAAY,EACZC,WAAY,OAEhBzF,QAAQoF,QAAQ,oBAEhBpF,QAAQ8C,KAAK,gBACbT,KAAK9B,YAAcmF,EAAAA,IAAqBJ,gBAAgBjD,KAAKkD,SAAU,IAChEjB,EACHjE,kBAAmB0C,EACnB5C,UAAW,SACXC,kBAAkB,EAClBuF,cAAc,EACdC,YAAa,YAEjB5F,QAAQoF,QAAQ,eAEpB,CAAE,MAAOS,GAEL7F,QAAQ6C,KAAK,sDAAuDgD,GAEpExD,KAAKK,gBAAkB2C,EAAAA,IAAcC,gBAAgBjD,KAAKkD,SAAU,CAChElF,kBAAmB0C,IAGvBV,KAAK9B,YAAcmF,EAAAA,IAAqBJ,gBAAgBjD,KAAKkD,SAAU,CACnEhC,OAAQ,OACRlD,kBAAmB0C,IAGvB/C,QAAQC,IAAI,mCAChB,CAEAoC,KAAKC,SAAW,CACdI,UAAWL,KAAKK,UAChBnC,MAAO8B,KAAK9B,MACZkC,QAASlD,UACP,IACM8C,KAAK9B,OAAOkC,eAAeJ,KAAK9B,MAAMkC,UACtCJ,KAAKK,WAAWD,eAAeJ,KAAKK,UAAUD,SACpD,CAAE,MAAOjC,GACPR,QAAQ6C,KAAK,0BAA2BrC,EAC1C,IAIJ6B,KAAKE,SAAWJ,EAChBb,EAAc,QACdE,EAAgB,SAChBxB,QAAQoF,QAAQ,qBAGhB,MAAMU,EAAkBC,aAAY,KACTrE,KAAKS,MAAQE,KAAKE,SACpBF,KAAKG,wBAC1BxC,QAAQC,IAAI,0CACZ+F,cAAcF,GACdzD,KAAKC,UAAUG,UACfJ,KAAKC,SAAW,KAClB,GACCD,KAAKG,uBAER,OAAOH,KAAKC,QACd,CAAE,MAAO9B,GACPc,EAAc,QACd,MAAM2E,EAAezF,EAAM0F,QAAQhB,SAAS,UAC1C,iBAAiB1E,EAAM0F,mCACvB1F,EAAM0F,QAER,MADA1E,EAAgB,QAASyE,GACnB,IAAI7C,MAAM6C,EAClB,CACF,EAIFrE,OAAOC,QAAQsE,UAAUC,aAAY7G,UAC/B0C,EAAuBK,WACzBtC,QAAQC,IAAI,kDACNgC,EAAuBK,SAASG,UACtCR,EAAuBK,SAAW,KACpC,IAIF,MAAM+D,EAAW9G,UACfS,QAAQC,IAAI,gCAAiCqG,GAC7C,IAEEC,WAAWC,iCAAmC,SAC9CxG,QAAQC,IAAI,gCACZ,MAAM,UAAEyC,EAAS,MAAEnC,SAAgB0B,EAAuBG,aAAa9B,IACrEN,QAAQC,IAAI,0BAA2BK,EAAI,IAE7CN,QAAQC,IAAI,2BAEZ,MAAMwG,EAAW,CAAC,CAAEC,KAAM,OAAQC,QAASL,IAC3CtG,QAAQC,IAAI,6BACZ,MAAM2G,EAASlE,EAAUmE,oBAAoBJ,EAAU,CACrDK,uBAAuB,EACvBC,aAAa,EACbtB,WAAY,OAUd,GANAzF,QAAQC,IAAI,mBAAoB,CAC9B+G,SAAUJ,EAAOK,WAAWjG,OAC5BkG,cAAeN,EAAOO,gBAAgBnG,SAIpC4F,EAAOK,WAAWjG,OAAS,KAC7B,MAAM,IAAIoC,MAAM,mDAGlBpD,QAAQC,IAAI,gCACZ,IACE,MAAM,UAAEmH,SAAoB7G,EAAM8F,SAAS,IACtCO,EACHS,WAAW,EACXC,eAAgB,IAChBC,eAAgB,EAChB9B,WAAY,KACZ+B,aAAc9E,EAAU8E,aACxBC,aAAc/E,EAAU+E,aACxBrG,oBACAsG,yBAAyB,EAEzBC,mBAAoB,IACpBC,qBAAsB,EACtBC,gBAAgB,IAGlB7H,QAAQC,IAAI,mCACZ,MAAM6H,EAAUpF,EAAUqF,aAAaX,EAAW,CAChDY,qBAAqB,IAIvB,OADAhI,QAAQC,IAAI,uBAAwB6H,EAAQ,IACrCA,EAAQ,EACjB,CAAE,MAAOG,GAEP,GAAIA,EAAS/B,SAAShB,SAAS,cAAe,CAC5ClF,QAAQQ,MAAM,gCAAiCyH,GAG/CjI,QAAQC,IAAI,+CACZ,MAAMiI,EAAkB,IACnBtB,EACHK,UAAWL,EAAOK,UAAUkB,OAAO,KACnChB,eAAgBP,EAAOO,eAAegB,OAAO,OAGzC,UAAEf,SAAoB7G,EAAM8F,SAAS,IACtC6B,EACHb,WAAW,EACXC,eAAgB,IAChBC,eAAgB,EAChB9B,WAAY,IACZ+B,aAAc9E,EAAU8E,aACxBC,aAAc/E,EAAU+E,aACxBrG,oBACAsG,yBAAyB,EACzBC,mBAAoB,IACpBC,qBAAsB,EACtBC,gBAAgB,IAOlB,OAJgBnF,EAAUqF,aAAaX,EAAW,CAChDY,qBAAqB,IAGR,GAAK,kDACtB,CACA,MAAMC,CACR,CACF,CAAE,MAAOzH,GASP,GARAR,QAAQQ,MAAM,8BAA+B,CAC3C2D,KAAM3D,EAAM2D,KACZ+B,QAAS1F,EAAM0F,QACfkC,MAAO5H,EAAM4H,MACbxB,OAAQN,GAAMtF,SAIZR,EAAM0F,SAAShB,SAAS,cAC1B,MAAM,IAAI9B,MAAM,oEAElB,MAAM5C,CACR,GAKFoB,OAAOC,QAAQwG,YAAYjC,aAAY7G,UAErCqC,OAAO0G,aAAaC,OAAO,CACzBC,GAAIrH,EACJsH,MAAO,qBACPC,SAAU,CAAC,qBAIiBjI,MAE5BT,QAAQC,IAAI,kCACZT,GAAmBuC,IACjBP,EAAgB,UAAW,CACzBC,OAAQ,cACRlB,MAAOwB,EAASxB,MAChB0C,KAAMlB,EAASkB,KACflB,SAAUA,EAASA,UACpB,IAEL,IAIFH,OAAO0G,aAAaK,UAAUvC,aAAY7G,MAAOqJ,EAAMC,KAErD,GADA7I,QAAQC,IAAI,wBAAyB2I,GACjCA,EAAKE,aAAe3H,GAAyByH,EAAKG,cAEtD,IAEE/I,QAAQC,IAAI,kCAAmC2I,EAAKG,eACpD,MAAMC,QAAe3C,EAASuC,EAAKG,eACnC/I,QAAQC,IAAI,oBAAqB+I,GAGjCpH,OAAOqH,UAAUC,cAAc,CAC7BC,OAAQ,CAAEC,MAAOP,EAAIL,IACrBa,KAAM,CAACL,GACPM,SAAWN,IAET,MAAMO,EAAgBC,SAASC,cAAc,OAC7CF,EAAcf,GAAK,yBACnBe,EAAcG,MAAMC,QAAU,wXAgB9B,MAAMC,EAASJ,SAASC,cAAc,OACtCG,EAAOF,MAAMC,QAAU,sLAOvBC,EAAOC,UAAY,sDAEnB,MAAMC,EAAWN,SAASC,cAAc,UACxCK,EAASC,YAAc,IACvBD,EAASJ,MAAMC,QAAU,sJAOzBG,EAASE,QAAU,IAAMT,EAAcU,SACvCL,EAAOM,YAAYJ,GAGnB,MAAMnD,EAAU6C,SAASC,cAAc,OACvC9C,EAAQ+C,MAAMC,QAAU,6HAQxB,MAAMQ,EAAcX,SAASC,cAAc,OAC3CU,EAAYT,MAAMC,QAAU,0IAM5BQ,EAAYJ,YAAcK,OAAOC,eAAeC,WAGhD,MAAMC,EAAaf,SAASC,cAAc,OAC1Cc,EAAWb,MAAMC,QAAU,4IAM3BY,EAAWR,YAAcf,EAEzBrC,EAAQuD,YAAYC,GACpBxD,EAAQuD,YAAYK,GAEpBhB,EAAcW,YAAYN,GAC1BL,EAAcW,YAAYvD,GAC1B6C,SAASgB,KAAKN,YAAYX,GAE1BvJ,QAAQC,IAAI,gCAA+B,GAGjD,CAAE,MAAOO,GACPR,QAAQQ,MAAM,qCAAsCA,GAEpDoB,OAAOqH,UAAUC,cAAc,CAC7BC,OAAQ,CAAEC,MAAOP,EAAIL,IACrBa,KAAM,CAAC7I,EAAM0F,SACboD,SAAWrD,IACTwE,MAAM,UAAYxE,EAAY,GAGpC,KAKFrE,OAAOC,QAAQ6I,UAAUtE,aAAY,CAACF,EAASyE,EAAQC,KACrD5K,QAAQC,IAAI,oBAAqBiG,GAEZ,iBAAjBA,EAAQtG,MACVgL,EAAa,CACXnJ,OAAQH,EACRS,SAAUR,KAEL,GAGc,aAAnB2E,EAAQ2E,SAGZ,iBACE,IACE7K,QAAQC,IAAI,mCAAoCiG,GAChD,MAAM8C,QAAe3C,EAASH,EAAQI,MACtCtG,QAAQC,IAAI,sBAAuB+I,GACnC4B,EAAa5B,EACf,CAAE,MAAOxI,GACPR,QAAQQ,MAAM,0BAA2BA,GACzCoK,EAAa,CAAEpK,MAAOA,EAAM0F,SAC9B,CACD,CAVD,IAYO,MAITtE,OAAOC,QAAQiJ,UAAU1E,aAAa2E,IAClB,UAAdA,EAAK5G,OACP4G,EAAKC,aAAa5E,aAAY,KAC5BpG,QAAQC,IAAI,qBAAoB,IAGlC8K,EAAKL,UAAUtE,aAAa6E,IACT,iBAAbA,EAAIrL,MACNmL,EAAKG,YAAY,CACfzJ,OAAQH,EACRS,SAAUR,GAEd,IAEJ,IAIFK,OAAOC,QAAQsJ,UAAU/E,aAAY,KACnCpG,QAAQC,IAAI,wBAAuB,IAIrC2B,OAAOC,QAAQsE,UAAUC,aAAY7G,UACnCS,QAAQC,IAAI,6BACRgC,EAAuBK,iBACnBL,EAAuBK,SAASG,UACtCR,EAAuBK,SAAW,KACpC,IAIFV,OAAOC,QAAQwG,YAAYjC,aAAY7G,UACrCS,QAAQC,IAAI,+BAAgCmL,EAAQC,QAGpDzJ,OAAO0G,aAAaC,OAAO,CACzBC,GAAIrH,EACJsH,MAAO,qBACPC,SAAU,CAAC,qBAIiBjI,MAE5BT,QAAQC,IAAI,kCACZT,GAAmBuC,IACjBP,EAAgB,UAAW,CACzBC,OAAQ,cACRlB,MAAOwB,EAASxB,MAChB0C,KAAMlB,EAASkB,KACflB,SAAUA,EAASA,UACpB,IAEL,G,gFC/jBEuJ,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqBxI,IAAjByI,EACH,OAAOA,EAAaC,QAGrB,IAAIC,EAASL,EAAyBE,GAAY,CAGjDE,QAAS,CAAC,GAOX,OAHAE,EAAoBJ,GAAUG,EAAQA,EAAOD,QAASH,GAG/CI,EAAOD,OACf,CAGAH,EAAoBM,EAAID,ELzBpB3M,EAAW,GACfsM,EAAoBO,EAAI,CAAC9C,EAAQ+C,EAAUC,EAAIC,KAC9C,IAAGF,EAAH,CAMA,IAAIG,EAAeC,IACnB,IAASC,EAAI,EAAGA,EAAInN,EAAS+B,OAAQoL,IAAK,CAGzC,IAFA,IAAKL,EAAUC,EAAIC,GAAYhN,EAASmN,GACpCC,GAAY,EACPC,EAAI,EAAGA,EAAIP,EAAS/K,OAAQsL,MACpB,EAAXL,GAAsBC,GAAgBD,IAAanM,OAAOiB,KAAKwK,EAAoBO,GAAG7K,OAAOsL,GAAShB,EAAoBO,EAAES,GAAKR,EAASO,MAC9IP,EAASS,OAAOF,IAAK,IAErBD,GAAY,EACTJ,EAAWC,IAAcA,EAAeD,IAG7C,GAAGI,EAAW,CACbpN,EAASuN,OAAOJ,IAAK,GACrB,IAAIK,EAAIT,SACEhJ,IAANyJ,IAAiBzD,EAASyD,EAC/B,CACD,CACA,OAAOzD,CAnBP,CAJCiD,EAAWA,GAAY,EACvB,IAAI,IAAIG,EAAInN,EAAS+B,OAAQoL,EAAI,GAAKnN,EAASmN,EAAI,GAAG,GAAKH,EAAUG,IAAKnN,EAASmN,GAAKnN,EAASmN,EAAI,GACrGnN,EAASmN,GAAK,CAACL,EAAUC,EAAIC,EAqBjB,EC1BV9M,EAAWW,OAAO4M,eAAkBC,GAAS7M,OAAO4M,eAAeC,GAASA,GAASA,EAAa,UAQtGpB,EAAoBqB,EAAI,SAASC,EAAOC,GAEvC,GADU,EAAPA,IAAUD,EAAQxK,KAAKwK,IAChB,EAAPC,EAAU,OAAOD,EACpB,GAAoB,iBAAVA,GAAsBA,EAAO,CACtC,GAAW,EAAPC,GAAaD,EAAME,WAAY,OAAOF,EAC1C,GAAW,GAAPC,GAAoC,mBAAfD,EAAMG,KAAqB,OAAOH,CAC5D,CACA,IAAII,EAAKnN,OAAOyI,OAAO,MACvBgD,EAAoBkB,EAAEQ,GACtB,IAAIC,EAAM,CAAC,EACXhO,EAAiBA,GAAkB,CAAC,KAAMC,EAAS,CAAC,GAAIA,EAAS,IAAKA,EAASA,IAC/E,IAAI,IAAIgO,EAAiB,EAAPL,GAAYD,EAAyB,iBAAXM,KAAyBjO,EAAekO,QAAQD,GAAUA,EAAUhO,EAASgO,GACxHrN,OAAOuN,oBAAoBF,GAASG,SAASf,GAASW,EAAIX,GAAO,IAAOM,EAAMN,KAI/E,OAFAW,EAAa,QAAI,IAAM,EACvB3B,EAAoBgC,EAAEN,EAAIC,GACnBD,CACR,EKxBA1B,EAAoBgC,EAAI,CAAC7B,EAAS8B,KACjC,IAAI,IAAIjB,KAAOiB,EACXjC,EAAoBkC,EAAED,EAAYjB,KAAShB,EAAoBkC,EAAE/B,EAASa,IAC5EzM,OAAO4N,eAAehC,EAASa,EAAK,CAAEoB,YAAY,EAAMC,IAAKJ,EAAWjB,IAE1E,ECNDhB,EAAoBsC,EAAI,WACvB,GAA0B,iBAAftH,WAAyB,OAAOA,WAC3C,IACC,OAAOlE,MAAQ,IAAIyL,SAAS,cAAb,EAChB,CAAE,MAAOlK,GACR,GAAsB,iBAAXwG,OAAqB,OAAOA,MACxC,CACA,CAPuB,GCAxBmB,EAAoBkC,EAAI,CAACd,EAAKoB,IAAUjO,OAAOkO,UAAUC,eAAeC,KAAKvB,EAAKoB,GCClFxC,EAAoBkB,EAAKf,IACH,oBAAXyC,QAA0BA,OAAOC,aAC1CtO,OAAO4N,eAAehC,EAASyC,OAAOC,YAAa,CAAEvB,MAAO,WAE7D/M,OAAO4N,eAAehC,EAAS,aAAc,CAAEmB,OAAO,GAAO,ECL9DtB,EAAoB8C,EAAI,I,MCAxB9C,EAAoB+C,EAAI9E,SAAS+E,SAAWC,KAAKC,SAASC,KAK1D,IAAIC,EAAkB,CACrB,IAAK,GAaNpD,EAAoBO,EAAEQ,EAAKsC,GAA0C,IAA7BD,EAAgBC,GAGxD,IAAIC,EAAuB,CAACC,EAA4BxO,KACvD,IAGIkL,EAAUoD,GAHT7C,EAAUgD,EAAalN,GAAWvB,EAGhB8L,EAAI,EAC3B,GAAGL,EAASiD,MAAMxG,GAAgC,IAAxBmG,EAAgBnG,KAAa,CACtD,IAAIgD,KAAYuD,EACZxD,EAAoBkC,EAAEsB,EAAavD,KACrCD,EAAoBM,EAAEL,GAAYuD,EAAYvD,IAGhD,GAAG3J,EAAS,IAAImH,EAASnH,EAAQ0J,EAClC,CAEA,IADGuD,GAA4BA,EAA2BxO,GACrD8L,EAAIL,EAAS/K,OAAQoL,IACzBwC,EAAU7C,EAASK,GAChBb,EAAoBkC,EAAEkB,EAAiBC,IAAYD,EAAgBC,IACrED,EAAgBC,GAAS,KAE1BD,EAAgBC,GAAW,EAE5B,OAAOrD,EAAoBO,EAAE9C,EAAO,EAGjCiG,EAAqBT,KAAoC,8BAAIA,KAAoC,+BAAK,GAC1GS,EAAmB3B,QAAQuB,EAAqBK,KAAK,KAAM,IAC3DD,EAAmBtL,KAAOkL,EAAqBK,KAAK,KAAMD,EAAmBtL,KAAKuL,KAAKD,G,KC7CvF,IAAIE,EAAsB5D,EAAoBO,OAAE9I,EAAW,CAAC,KAAK,IAAOuI,EAAoB,OAC5F4D,EAAsB5D,EAAoBO,EAAEqD,E","sources":["webpack://browser-extension/webpack/runtime/chunk loaded","webpack://browser-extension/webpack/runtime/create fake namespace object","webpack://browser-extension/./src/model-cache.js","webpack://browser-extension/./src/constants.js","webpack://browser-extension/./src/background.js","webpack://browser-extension/webpack/bootstrap","webpack://browser-extension/webpack/runtime/define property getters","webpack://browser-extension/webpack/runtime/global","webpack://browser-extension/webpack/runtime/hasOwnProperty shorthand","webpack://browser-extension/webpack/runtime/make namespace object","webpack://browser-extension/webpack/runtime/publicPath","webpack://browser-extension/webpack/runtime/jsonp chunk loading","webpack://browser-extension/webpack/startup"],"sourcesContent":["var deferred = [];\n__webpack_require__.O = (result, chunkIds, fn, priority) => {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar [chunkIds, fn, priority] = deferred[i];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every((key) => (__webpack_require__.O[key](chunkIds[j])))) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","var getProto = Object.getPrototypeOf ? (obj) => (Object.getPrototypeOf(obj)) : (obj) => (obj.__proto__);\nvar leafPrototypes;\n// create a fake namespace object\n// mode & 1: value is a module id, require it\n// mode & 2: merge all properties of value into the ns\n// mode & 4: return value when already ns object\n// mode & 16: return value when it's Promise-like\n// mode & 8|1: behave like require\n__webpack_require__.t = function(value, mode) {\n\tif(mode & 1) value = this(value);\n\tif(mode & 8) return value;\n\tif(typeof value === 'object' && value) {\n\t\tif((mode & 4) && value.__esModule) return value;\n\t\tif((mode & 16) && typeof value.then === 'function') return value;\n\t}\n\tvar ns = Object.create(null);\n\t__webpack_require__.r(ns);\n\tvar def = {};\n\tleafPrototypes = leafPrototypes || [null, getProto({}), getProto([]), getProto(getProto)];\n\tfor(var current = mode & 2 && value; typeof current == 'object' && !~leafPrototypes.indexOf(current); current = getProto(current)) {\n\t\tObject.getOwnPropertyNames(current).forEach((key) => (def[key] = () => (value[key])));\n\t}\n\tdef['default'] = () => (value);\n\t__webpack_require__.d(ns, def);\n\treturn ns;\n};","// model-cache.js - Handles caching and pre-downloading of models\r\nimport { pipeline } from \"@huggingface/transformers\";\r\n\r\nconst MODELS = {\r\n    TEXT: \"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\",\r\n    ASR: \"onnx-community/whisper-tiny.en\"\r\n};\r\n\r\nexport async function preDownloadModels(progressCallback) {\r\n    const cache = await caches.open('model-cache-v1');\r\n    \r\n    for (const [type, modelId] of Object.entries(MODELS)) {\r\n        try {\r\n            console.log(`Pre-downloading ${type} model: ${modelId}`);\r\n            \r\n            // Download model files to cache\r\n            const modelFiles = await pipeline(\r\n                type === 'ASR' ? \"automatic-speech-recognition\" : \"text-generation\",\r\n                modelId,\r\n                {\r\n                    cache_dir: 'models',\r\n                    local_files_only: false,\r\n                    progress_callback: (data) => {\r\n                        if (progressCallback) {\r\n                            progressCallback({\r\n                                model: type,\r\n                                ...data\r\n                            });\r\n                        }\r\n                    }\r\n                }\r\n            );\r\n            \r\n            console.log(`Successfully cached ${type} model`);\r\n            \r\n        } catch (error) {\r\n            console.error(`Error pre-downloading ${type} model:`, error);\r\n        }\r\n    }\r\n}\r\n\r\n// Function to check if models are cached\r\nexport async function areModelsCached() {\r\n    try {\r\n        const cache = await caches.open('model-cache-v1');\r\n        const cached = await Promise.all(\r\n            Object.values(MODELS).map(async (modelId) => {\r\n                const files = await cache.keys(`models/${modelId}/*`);\r\n                return files.length > 0;\r\n            })\r\n        );\r\n        return cached.every(Boolean);\r\n    } catch (error) {\r\n        console.error('Error checking model cache:', error);\r\n        return false;\r\n    }\r\n}","export const CONTEXT_MENU_ITEM_ID = \"generate-from-selection\";\r\nexport const ACTION_NAME = \"generate\";\r\n","// background.js - Handles requests from the UI, runs the model, then sends back a response\r\nimport \"@huggingface/transformers\";\r\nimport {\r\n  AutoTokenizer,\r\n  AutoModelForCausalLM,\r\n  TextStreamer,\r\n  InterruptableStoppingCriteria,\r\n  pipeline\r\n} from \"@huggingface/transformers\";\r\nimport { preDownloadModels, areModelsCached } from \"./model-cache.js\";\r\nimport { CONTEXT_MENU_ITEM_ID } from \"./constants.js\";\r\n\r\n// Set up the stopping criteria for interrupting generation if needed\r\nconst stopping_criteria = new InterruptableStoppingCriteria();\r\n\r\nlet modelStatus = 'uninitialized';\r\nlet loadingProgress = 0;\r\n\r\n// Note: ONNX Runtime will show warnings about some operations being assigned to CPU.\r\n// This is expected and optimal behavior - certain operations (especially shape-related ones) \r\n// are deliberately run on CPU as they perform better there, even when using WebGPU\r\n// as the primary execution provider.\r\n\r\n// Function to broadcast status to all popup windows\r\nfunction broadcastStatus(status, data = null) {\r\n    // Add timestamp to loading status messages\r\n    if (status === 'loading' && data?.status) {\r\n        console.log(`[${new Date().toISOString()}] Loading progress:`, data);\r\n    }\r\n    chrome.runtime.sendMessage({\r\n        status,\r\n        data,\r\n        progress: loadingProgress\r\n    }).catch(() => {\r\n        // Ignore errors when no popups are open\r\n    });\r\n}\r\n\r\n/**\r\n * This class uses the Singleton pattern to enable lazy-loading of the pipeline\r\n */\r\nclass TextGenerationPipeline {\r\n  static model_id = \"onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX\";\r\n  static instance = null;\r\n  static lastUsed = Date.now();\r\n  static memoryCleanupInterval = 5 * 60 * 1000; // 5 minutes\r\n  \r\n  static async getInstance(progress_callback = null) {\r\n    const now = Date.now();\r\n    \r\n    // Check if we need to clean up old instance\r\n    if (this.instance && (now - this.lastUsed > this.memoryCleanupInterval)) {\r\n      console.log('Cleaning up old model instance to free memory...');\r\n      try {\r\n        // Cleanup ONNX Runtime resources\r\n        if (this.instance.model?.destroy) {\r\n          await this.instance.model.destroy();\r\n        }\r\n        if (this.instance.tokenizer?.destroy) {\r\n          await this.instance.tokenizer.destroy();\r\n        }\r\n        this.instance = null;\r\n        // Force garbage collection if available\r\n        if (global.gc) {\r\n          global.gc();\r\n        }\r\n      } catch (error) {\r\n        console.warn('Error cleaning up model:', error);\r\n      }\r\n    }\r\n\r\n    if (this.instance) {\r\n      this.lastUsed = now;\r\n      broadcastStatus('ready');\r\n      return this.instance;\r\n    }\r\n\r\n    try {\r\n      console.time('totalModelLoading');\r\n      modelStatus = 'loading';\r\n      broadcastStatus('loading');\r\n\r\n      const updateProgress = (data) => {\r\n          if (data.progress !== undefined) {\r\n              loadingProgress = data.progress;\r\n              console.log(`[${new Date().toISOString()}] Loading step:`, {\r\n                  file: data.file,\r\n                  status: data.status,\r\n                  progress: data.progress\r\n              });\r\n              broadcastStatus('loading', data);\r\n          }\r\n          if (progress_callback) progress_callback(data);\r\n      };\r\n\r\n      // First try loading with WebGPU\r\n      try {\r\n          console.time('webgpuInit');\r\n          if (!navigator.gpu) {\r\n              throw new Error(\"WebGPU is not supported in this browser\");\r\n          }\r\n\r\n          const adapter = await navigator.gpu.requestAdapter();\r\n          if (!adapter) {\r\n              throw new Error(\"Failed to get WebGPU adapter\");\r\n          }\r\n\r\n          // Try to get device with f16 support, but don't require it\r\n          let device;\r\n          let features = [];\r\n          try {\r\n            device = await adapter.requestDevice({\r\n              requiredFeatures: ['shader-f16']\r\n            });\r\n            features.push('shader-f16');\r\n          } catch (e) {\r\n            console.log('F16 shaders not supported, using standard WebGPU');\r\n            device = await adapter.requestDevice();\r\n          }\r\n\r\n          // Set memory limits based on available GPU memory\r\n          const gpuLimits = {\r\n            maxStorageBufferBindingSize: Math.min(\r\n              adapter.limits.maxStorageBufferBindingSize,\r\n              2147483648 // 2GB max\r\n            ),\r\n            maxBufferSize: Math.min(\r\n              adapter.limits.maxBufferSize,\r\n              2147483648 // 2GB max\r\n            )\r\n          };\r\n\r\n          console.log('Using WebGPU adapter:', {\r\n            name: adapter.name,\r\n            features: Array.from(adapter.features),\r\n            limits: gpuLimits\r\n          });\r\n\r\n          // ONNX Runtime config based on available features\r\n          const onnxConfig = {\r\n            executionProviders: [\"webgpu\", \"cpu\"],\r\n            graphOptimizationLevel: 99,\r\n            enableMemoryPattern: false,\r\n            executionMode: \"sequential\",\r\n            enableCpuMemArena: false,\r\n            webgpu: {\r\n              gpuDevice: device,\r\n              preferredLayout: \"nchw\",\r\n              deviceType: \"discrete\",\r\n              shaderPreprocess: true,\r\n              useHostMemory: false\r\n            }\r\n          };\r\n\r\n          // Add f16 settings only if supported\r\n          if (features.includes('shader-f16')) {\r\n            onnxConfig.webgpu.shader_features = ['f16'];\r\n          }\r\n\r\n          console.timeEnd('webgpuInit');\r\n\r\n          console.time('tokenizerLoading');\r\n          this.tokenizer = await AutoTokenizer.from_pretrained(this.model_id, {\r\n              progress_callback: updateProgress,\r\n              cache_dir: 'models',\r\n              local_files_only: true, // Try to use cached files first\r\n              truncation: true,\r\n              max_length: 2048\r\n          });\r\n          console.timeEnd('tokenizerLoading');\r\n\r\n          console.time('modelLoading');\r\n          this.model = await AutoModelForCausalLM.from_pretrained(this.model_id, {\r\n              ...onnxConfig,\r\n              progress_callback: updateProgress,\r\n              cache_dir: 'models',\r\n              local_files_only: true, // Try to use cached files first\r\n              load_in_8bit: true, // Use 8-bit quantization\r\n              torch_dtype: 'float16' // Use fp16 precision\r\n          });\r\n          console.timeEnd('modelLoading');\r\n\r\n      } catch (gpuError) {\r\n          // If WebGPU fails, fall back to WASM\r\n          console.warn('WebGPU initialization failed, falling back to WASM:', gpuError);\r\n          \r\n          this.tokenizer = await AutoTokenizer.from_pretrained(this.model_id, {\r\n              progress_callback: updateProgress,\r\n          });\r\n\r\n          this.model = await AutoModelForCausalLM.from_pretrained(this.model_id, {\r\n              device: \"wasm\",\r\n              progress_callback: updateProgress\r\n          });\r\n          \r\n          console.log('Successfully initialized on WASM');\r\n      }\r\n\r\n      this.instance = { \r\n        tokenizer: this.tokenizer, \r\n        model: this.model,\r\n        destroy: async () => {\r\n          try {\r\n            if (this.model?.destroy) await this.model.destroy();\r\n            if (this.tokenizer?.destroy) await this.tokenizer.destroy();\r\n          } catch (error) {\r\n            console.warn('Error destroying model:', error);\r\n          }\r\n        }\r\n      };\r\n      \r\n      this.lastUsed = now;\r\n      modelStatus = 'ready';\r\n      broadcastStatus('ready');\r\n      console.timeEnd('totalModelLoading');\r\n      \r\n      // Set up automatic cleanup\r\n      const cleanupInterval = setInterval(() => {\r\n        const timeSinceLastUse = Date.now() - this.lastUsed;\r\n        if (timeSinceLastUse > this.memoryCleanupInterval) {\r\n          console.log('Auto-cleaning unused model instance...');\r\n          clearInterval(cleanupInterval);\r\n          this.instance?.destroy();\r\n          this.instance = null;\r\n        }\r\n      }, this.memoryCleanupInterval);\r\n      \r\n      return this.instance;\r\n    } catch (error) {\r\n      modelStatus = 'error';\r\n      const errorMessage = error.message.includes('WebGPU') ? \r\n        `WebGPU error: ${error.message}. Falling back to WASM...` :\r\n        error.message;\r\n      broadcastStatus('error', errorMessage);\r\n      throw new Error(errorMessage);\r\n    }\r\n  }\r\n}\r\n\r\n// Add cleanup on extension unload\r\nchrome.runtime.onSuspend.addListener(async () => {\r\n  if (TextGenerationPipeline.instance) {\r\n    console.log('Cleaning up model on extension unload...');\r\n    await TextGenerationPipeline.instance.destroy();\r\n    TextGenerationPipeline.instance = null;\r\n  }\r\n});\r\n\r\n// Create generic generate function that will be reused\r\nconst generate = async (text) => {\r\n  console.log('Starting text generation for:', text);\r\n  try {\r\n    // Get the pipeline instance with correct WASM path\r\n    globalThis.__TRANSFORMER_WORKER_WASM_PATH__ = '/wasm/';\r\n    console.log('Getting pipeline instance...');\r\n    const { tokenizer, model } = await TextGenerationPipeline.getInstance((data) => {\r\n      console.log(\"Loading model progress:\", data);\r\n    });\r\n    console.log('Pipeline instance ready');\r\n\r\n    const messages = [{ role: \"user\", content: text }];\r\n    console.log('Applying chat template...');\r\n    const inputs = tokenizer.apply_chat_template(messages, {\r\n      add_generation_prompt: true,\r\n      return_dict: true,\r\n      max_length: 2048 // Limit input length to avoid memory issues\r\n    });\r\n\r\n    // Log the tokenized input for debugging\r\n    console.log('Tokenized input:', {\r\n      inputIds: inputs.input_ids?.length,\r\n      attentionMask: inputs.attention_mask?.length,\r\n    });\r\n\r\n    // Add memory management - ensure we're not exceeding safe limits\r\n    if (inputs.input_ids?.length > 2048) {\r\n      throw new Error('Input is too long - please provide shorter text');\r\n    }\r\n\r\n    console.log('Starting model generation...');\r\n    try {\r\n      const { sequences } = await model.generate({\r\n        ...inputs,\r\n        do_sample: false,\r\n        max_new_tokens: 512,\r\n        min_new_tokens: 1, // Ensure we generate at least something\r\n        max_length: 2048,\r\n        pad_token_id: tokenizer.pad_token_id,\r\n        eos_token_id: tokenizer.eos_token_id,\r\n        stopping_criteria,\r\n        return_dict_in_generate: true,\r\n        // Add safety parameters\r\n        repetition_penalty: 1.2,\r\n        no_repeat_ngram_size: 3,\r\n        early_stopping: true\r\n      });\r\n\r\n      console.log('Decoding generated sequences...');\r\n      const decoded = tokenizer.batch_decode(sequences, {\r\n        skip_special_tokens: true,\r\n      });\r\n\r\n      console.log('Generation complete:', decoded[0]);\r\n      return decoded[0];\r\n    } catch (genError) {\r\n      // Handle specific ONNX runtime errors\r\n      if (genError.message?.includes('1879778072')) {\r\n        console.error('ONNX memory allocation error:', genError);\r\n        \r\n        // Try to recover by clearing model cache and retrying with smaller context\r\n        console.log('Attempting recovery with smaller context...');\r\n        const truncatedInputs = {\r\n          ...inputs,\r\n          input_ids: inputs.input_ids.slice(-512), // Take last 512 tokens\r\n          attention_mask: inputs.attention_mask.slice(-512)\r\n        };\r\n\r\n        const { sequences } = await model.generate({\r\n          ...truncatedInputs,\r\n          do_sample: false,\r\n          max_new_tokens: 256, // Reduce output size\r\n          min_new_tokens: 1,\r\n          max_length: 768,\r\n          pad_token_id: tokenizer.pad_token_id,\r\n          eos_token_id: tokenizer.eos_token_id,\r\n          stopping_criteria,\r\n          return_dict_in_generate: true,\r\n          repetition_penalty: 1.2,\r\n          no_repeat_ngram_size: 3,\r\n          early_stopping: true\r\n        });\r\n\r\n        const decoded = tokenizer.batch_decode(sequences, {\r\n          skip_special_tokens: true,\r\n        });\r\n\r\n        return decoded[0] + \"\\n\\n(Note: Response was truncated due to length)\";\r\n      }\r\n      throw genError;\r\n    }\r\n  } catch (error) {\r\n    console.error('Error in generate function:', {\r\n      name: error.name,\r\n      message: error.message,\r\n      stack: error.stack,\r\n      inputs: text?.length\r\n    });\r\n    \r\n    // Provide more helpful error messages\r\n    if (error.message?.includes('1879778072')) {\r\n      throw new Error('The model ran out of memory. Please try with shorter input text.');\r\n    }\r\n    throw error;\r\n  }\r\n};\r\n\r\n////////////////////// 1. Context Menus //////////////////////\r\n// Handle extension installation\r\nchrome.runtime.onInstalled.addListener(async (details) => {\r\n  // Create context menu\r\n  chrome.contextMenus.create({\r\n    id: CONTEXT_MENU_ITEM_ID,\r\n    title: 'Generate from \"%s\"',\r\n    contexts: [\"selection\"],\r\n  });\r\n\r\n  // Start pre-downloading models if not cached\r\n  const modelsAreCached = await areModelsCached();\r\n  if (!modelsAreCached) {\r\n    console.log('Starting model pre-download...');\r\n    preDownloadModels((progress) => {\r\n      broadcastStatus('loading', {\r\n        status: 'downloading',\r\n        model: progress.model,\r\n        file: progress.file,\r\n        progress: progress.progress\r\n      });\r\n    });\r\n  }\r\n});\r\n\r\n// Handle context menu clicks\r\nchrome.contextMenus.onClicked.addListener(async (info, tab) => {\r\n  console.log('Context menu clicked:', info);\r\n  if (info.menuItemId !== CONTEXT_MENU_ITEM_ID || !info.selectionText) return;\r\n\r\n  try {\r\n    // Generate text from the selected text\r\n    console.log('Generating text from selection:', info.selectionText);\r\n    const result = await generate(info.selectionText);\r\n    console.log('Generated result:', result);\r\n\r\n    // Show the result in the webpage\r\n    chrome.scripting.executeScript({\r\n      target: { tabId: tab.id },\r\n      args: [result],\r\n      function: (result) => {\r\n        // Create a floating div to show the result\r\n        const chatContainer = document.createElement('div');\r\n        chatContainer.id = 'copilot-chat-container';\r\n        chatContainer.style.cssText = `\r\n          position: fixed;\r\n          top: 20px;\r\n          right: 20px;\r\n          width: 400px;\r\n          max-height: 80vh;\r\n          background: white;\r\n          border: 1px solid #ccc;\r\n          border-radius: 8px;\r\n          box-shadow: 0 2px 10px rgba(0,0,0,0.1);\r\n          z-index: 10000;\r\n          display: flex;\r\n          flex-direction: column;\r\n        `;\r\n\r\n        // Add header with title and close button\r\n        const header = document.createElement('div');\r\n        header.style.cssText = `\r\n          padding: 12px;\r\n          border-bottom: 1px solid #eee;\r\n          display: flex;\r\n          justify-content: space-between;\r\n          align-items: center;\r\n        `;\r\n        header.innerHTML = '<span style=\"font-weight: bold;\">AI Response</span>';\r\n\r\n        const closeBtn = document.createElement('button');\r\n        closeBtn.textContent = '×';\r\n        closeBtn.style.cssText = `\r\n          border: none;\r\n          background: none;\r\n          cursor: pointer;\r\n          font-size: 20px;\r\n          padding: 0 4px;\r\n        `;\r\n        closeBtn.onclick = () => chatContainer.remove();\r\n        header.appendChild(closeBtn);\r\n\r\n        // Add chat content\r\n        const content = document.createElement('div');\r\n        content.style.cssText = `\r\n          padding: 16px;\r\n          overflow-y: auto;\r\n          font-size: 14px;\r\n          line-height: 1.5;\r\n        `;\r\n\r\n        // Add user message\r\n        const userMessage = document.createElement('div');\r\n        userMessage.style.cssText = `\r\n          margin-bottom: 12px;\r\n          padding: 8px 12px;\r\n          background: #f0f0f0;\r\n          border-radius: 8px;\r\n        `;\r\n        userMessage.textContent = window.getSelection().toString();\r\n\r\n        // Add AI response\r\n        const aiResponse = document.createElement('div');\r\n        aiResponse.style.cssText = `\r\n          padding: 8px 12px;\r\n          background: #e3f2fd;\r\n          border-radius: 8px;\r\n          white-space: pre-wrap;\r\n        `;\r\n        aiResponse.textContent = result;\r\n\r\n        content.appendChild(userMessage);\r\n        content.appendChild(aiResponse);\r\n\r\n        chatContainer.appendChild(header);\r\n        chatContainer.appendChild(content);\r\n        document.body.appendChild(chatContainer);\r\n\r\n        console.log('Chat UI created and displayed');\r\n      },\r\n    });\r\n  } catch (error) {\r\n    console.error('Error handling context menu click:', error);\r\n    // Show error to user\r\n    chrome.scripting.executeScript({\r\n      target: { tabId: tab.id },\r\n      args: [error.message],\r\n      function: (errorMessage) => {\r\n        alert('Error: ' + errorMessage);\r\n      },\r\n    });\r\n  }\r\n});\r\n\r\n////////////////////// 2. Message Events /////////////////////\r\n// Handle messages from the popup or other parts of the extension\r\nchrome.runtime.onMessage.addListener((message, sender, sendResponse) => {\r\n  console.log('Received message:', message);\r\n  \r\n  if (message.type === 'check_status') {\r\n    sendResponse({\r\n      status: modelStatus,\r\n      progress: loadingProgress\r\n    });\r\n    return true;\r\n  }\r\n\r\n  if (message.action !== \"generate\") return false;\r\n\r\n  // Handle generate action\r\n  (async function () {\r\n    try {\r\n      console.log('Generating response for message:', message);\r\n      const result = await generate(message.text);\r\n      console.log('Generated response:', result);\r\n      sendResponse(result);\r\n    } catch (error) {\r\n      console.error('Error handling message:', error);\r\n      sendResponse({ error: error.message });\r\n    }\r\n  })();\r\n\r\n  return true; // Will respond asynchronously\r\n});\r\n\r\n// Listen for connection-related events\r\nchrome.runtime.onConnect.addListener((port) => {\r\n  if (port.name === 'popup') {\r\n    port.onDisconnect.addListener(() => {\r\n      console.log('Popup disconnected');\r\n    });\r\n    \r\n    port.onMessage.addListener((msg) => {\r\n      if (msg.type === 'check_status') {\r\n        port.postMessage({\r\n          status: modelStatus,\r\n          progress: loadingProgress\r\n        });\r\n      }\r\n    });\r\n  }\r\n});\r\n\r\n// Keep service worker alive\r\nchrome.runtime.onStartup.addListener(() => {\r\n  console.log('Extension starting up');\r\n});\r\n\r\n// Ensure cleanup on unload\r\nchrome.runtime.onSuspend.addListener(async () => {\r\n  console.log('Extension being suspended');\r\n  if (TextGenerationPipeline.instance) {\r\n    await TextGenerationPipeline.instance.destroy();\r\n    TextGenerationPipeline.instance = null;\r\n  }\r\n});\r\n\r\n// Handle installed/updated events\r\nchrome.runtime.onInstalled.addListener(async (details) => {\r\n  console.log('Extension installed/updated:', details.reason);\r\n  \r\n  // Create context menu\r\n  chrome.contextMenus.create({\r\n    id: CONTEXT_MENU_ITEM_ID,\r\n    title: 'Generate from \"%s\"',\r\n    contexts: [\"selection\"],\r\n  });\r\n\r\n  // Start pre-downloading models if not cached\r\n  const modelsAreCached = await areModelsCached();\r\n  if (!modelsAreCached) {\r\n    console.log('Starting model pre-download...');\r\n    preDownloadModels((progress) => {\r\n      broadcastStatus('loading', {\r\n        status: 'downloading',\r\n        model: progress.model,\r\n        file: progress.file,\r\n        progress: progress.progress\r\n      });\r\n    });\r\n  }\r\n});\r\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.g = (function() {\n\tif (typeof globalThis === 'object') return globalThis;\n\ttry {\n\t\treturn this || new Function('return this')();\n\t} catch (e) {\n\t\tif (typeof window === 'object') return window;\n\t}\n})();","__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.p = \"/\";","__webpack_require__.b = document.baseURI || self.location.href;\n\n// object to store loaded and loading chunks\n// undefined = chunk not loaded, null = chunk preloaded/prefetched\n// [resolve, reject, Promise] = chunk loading, 0 = chunk loaded\nvar installedChunks = {\n\t471: 0\n};\n\n// no chunk on demand loading\n\n// no prefetching\n\n// no preloaded\n\n// no HMR\n\n// no HMR manifest\n\n__webpack_require__.O.j = (chunkId) => (installedChunks[chunkId] === 0);\n\n// install a JSONP callback for chunk loading\nvar webpackJsonpCallback = (parentChunkLoadingFunction, data) => {\n\tvar [chunkIds, moreModules, runtime] = data;\n\t// add \"moreModules\" to the modules object,\n\t// then flag all \"chunkIds\" as loaded and fire callback\n\tvar moduleId, chunkId, i = 0;\n\tif(chunkIds.some((id) => (installedChunks[id] !== 0))) {\n\t\tfor(moduleId in moreModules) {\n\t\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t\t}\n\t\t}\n\t\tif(runtime) var result = runtime(__webpack_require__);\n\t}\n\tif(parentChunkLoadingFunction) parentChunkLoadingFunction(data);\n\tfor(;i < chunkIds.length; i++) {\n\t\tchunkId = chunkIds[i];\n\t\tif(__webpack_require__.o(installedChunks, chunkId) && installedChunks[chunkId]) {\n\t\t\tinstalledChunks[chunkId][0]();\n\t\t}\n\t\tinstalledChunks[chunkId] = 0;\n\t}\n\treturn __webpack_require__.O(result);\n}\n\nvar chunkLoadingGlobal = self[\"webpackChunkbrowser_extension\"] = self[\"webpackChunkbrowser_extension\"] || [];\nchunkLoadingGlobal.forEach(webpackJsonpCallback.bind(null, 0));\nchunkLoadingGlobal.push = webpackJsonpCallback.bind(null, chunkLoadingGlobal.push.bind(chunkLoadingGlobal));","// startup\n// Load entry module and return exports\n// This entry module depends on other loaded chunks and execution need to be delayed\nvar __webpack_exports__ = __webpack_require__.O(undefined, [96], () => (__webpack_require__(711)))\n__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n"],"names":["deferred","leafPrototypes","getProto","MODELS","TEXT","ASR","async","preDownloadModels","progressCallback","caches","open","type","modelId","Object","entries","console","log","pipeline","cache_dir","local_files_only","progress_callback","data","model","error","areModelsCached","cache","Promise","all","values","map","keys","length","every","Boolean","CONTEXT_MENU_ITEM_ID","stopping_criteria","InterruptableStoppingCriteria","modelStatus","loadingProgress","broadcastStatus","status","Date","toISOString","chrome","runtime","sendMessage","progress","catch","TextGenerationPipeline","static","now","getInstance","this","instance","lastUsed","memoryCleanupInterval","destroy","tokenizer","global","gc","warn","time","updateProgress","undefined","file","navigator","gpu","Error","adapter","requestAdapter","device","features","requestDevice","requiredFeatures","push","e","gpuLimits","maxStorageBufferBindingSize","Math","min","limits","maxBufferSize","name","Array","from","onnxConfig","executionProviders","graphOptimizationLevel","enableMemoryPattern","executionMode","enableCpuMemArena","webgpu","gpuDevice","preferredLayout","deviceType","shaderPreprocess","useHostMemory","includes","shader_features","timeEnd","AutoTokenizer","from_pretrained","model_id","truncation","max_length","AutoModelForCausalLM","load_in_8bit","torch_dtype","gpuError","cleanupInterval","setInterval","clearInterval","errorMessage","message","onSuspend","addListener","generate","text","globalThis","__TRANSFORMER_WORKER_WASM_PATH__","messages","role","content","inputs","apply_chat_template","add_generation_prompt","return_dict","inputIds","input_ids","attentionMask","attention_mask","sequences","do_sample","max_new_tokens","min_new_tokens","pad_token_id","eos_token_id","return_dict_in_generate","repetition_penalty","no_repeat_ngram_size","early_stopping","decoded","batch_decode","skip_special_tokens","genError","truncatedInputs","slice","stack","onInstalled","contextMenus","create","id","title","contexts","onClicked","info","tab","menuItemId","selectionText","result","scripting","executeScript","target","tabId","args","function","chatContainer","document","createElement","style","cssText","header","innerHTML","closeBtn","textContent","onclick","remove","appendChild","userMessage","window","getSelection","toString","aiResponse","body","alert","onMessage","sender","sendResponse","action","onConnect","port","onDisconnect","msg","postMessage","onStartup","details","reason","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","exports","module","__webpack_modules__","m","O","chunkIds","fn","priority","notFulfilled","Infinity","i","fulfilled","j","key","splice","r","getPrototypeOf","obj","t","value","mode","__esModule","then","ns","def","current","indexOf","getOwnPropertyNames","forEach","d","definition","o","defineProperty","enumerable","get","g","Function","prop","prototype","hasOwnProperty","call","Symbol","toStringTag","p","b","baseURI","self","location","href","installedChunks","chunkId","webpackJsonpCallback","parentChunkLoadingFunction","moreModules","some","chunkLoadingGlobal","bind","__webpack_exports__"],"sourceRoot":""}